{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a3c818-7de9-4d6f-8f65-2f0eade4ff7a",
   "metadata": {},
   "source": [
    "# Vector store Llama index\n",
    "\n",
    "# Vector store and index in Llama index\n",
    "\n",
    "When preparing data for the Llama Index, it's essential to understand and utilize the provided utilities effectively. Let's explore how these components work together, specifically focusing on the `StorageContext`, `ServiceContext`, `Document`, and `VectorStoreIndex`, and how they are applied in the indexing process.\n",
    "\n",
    "## ServiceContext\n",
    "\n",
    "- Purpose: The `ServiceContext` serves as a container for the LlamaIndex index and query classes. It provides the necessary context for the retrieval system, ensuring that your queries interact seamlessly with the indexed data.\n",
    "- Source Code: [ServiceContext Source](https://github.com/run-llama/llama_index/blob/main/llama_index/service_context.py)\n",
    "- Parameters: While the `ServiceContext` supports various parameters (see the full list [here](https://github.com/run-llama/llama_index/blob/2b74fccadb87701eff91bf4ce315829fc3fd9e62/llama_index/service_context.py#L85)), we focus on the most commonly used ones to configure the LLM, embedding model, and text chunking method:\n",
    "  - `embed_model`: The embedding model used for vectorizing the text.\n",
    "  - `llm`: The Large Language Model used, which by default is OpenAI's model (details [here](https://github.com/run-llama/llama_index/blob/2b74fccadb87701eff91bf4ce315829fc3fd9e62/llama_index/llms/utils.py#L16)).\n",
    "  - `text_splitter`: The method used to chunk the text. We use a `SentenceSplitter` with a `chunk_size` of 600 tokens and a chunk_overlap of 90 tokens. Given the distribution of document sizes in your dataset, these values are chosen to ensure that the documents are split into manageable chunks without losing context across chunks.\n",
    "\n",
    "## Document\n",
    "\n",
    "- Purpose: The `Document` class is used to convert and store documents in a format compatible with the Llama Index. It also allows adding metadata to improve searchability within the documents.\n",
    "- Source Code: [Document Source](https://github.com/run-llama/llama_index/blob/2b74fccadb87701eff91bf4ce315829fc3fd9e62/llama_index/schema.py#L590)\n",
    "Metadata: We enrich the documents with metadata such as 'communities', 'industry', 'interest', and 'title', which aids in more nuanced searches and better-organized indexing.\n",
    "\n",
    "## StorageContext\n",
    "\n",
    "- Purpose: The StorageContext acts as a centralized container for storing nodes, indices, and vectors. It's essentially the backbone of your data storage strategy within the Llama Index.\n",
    "- Source Code: [StorageContext Source](https://github.com/run-llama/llama_index/blob/main/llama_index/storage/storage_context.py)\n",
    "\n",
    "## VectorStoreIndex\n",
    "\n",
    "- Purpose: The `VectorStoreIndex` is a fundamental component of the retrieval-augmented generation (RAG) system. It's designed to efficiently store and retrieve vectors representing chunks of text, facilitating quick and relevant responses to queries.\n",
    "- Source Code: [VectorStoreIndex Source](https://github.com/run-llama/llama_index/blob/main/llama_index/indices/vector_store/base.py)\n",
    "\n",
    "## Indexing Process:\n",
    "\n",
    "The indexing process is central to the LlamaIndex. It involves converting documents into nodes and storing them in a structured manner to facilitate efficient retrieval. Each document is processed through a `text_splitter` before being vectorized using the `embed_model`. These processed documents are then stored in a `VectorStoreIndex`. The code snippet provided demonstrates this process, from setting up the `text_splitter` and `embed_model` to creating and populating the `VectorStoreIndex`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635893c-553c-4ab5-8379-f09fae3cf8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import Document\n",
    "from llama_index.node_parser import SentenceSplitter, get_leaf_nodes, get_root_nodes\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage import StorageContext\n",
    "from llama_index import load_index_from_storage, VectorStoreIndex, ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be5557e-45d6-4756-aa0f-13a37b8ed7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embed_model = HuggingFaceEmbedding(model_name=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad0f58-db5a-4e46-979a-1047fe0b15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project = 'XX'\n",
    "folder_vector = 'XX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4d911-efe2-417a-b580-2862f53ed83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(path_project, folder_vector), exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa6361f-0df3-4943-821b-fbbab1d93071",
   "metadata": {},
   "source": [
    "We have the option to either build a new storage from scratch or load an existing one from a persistent storage.\n",
    "\n",
    "\n",
    "### Building a New Storage\n",
    "\n",
    "- Setting Up the Service Context: Initially, `service_context_with_splitter` is defined using `ServiceContext.from_defaults`. This context is configured with the embedding model (`embed_model`), the large language model (`llm`), and the text splitter (`text_splitter`) which chunks the text data.\n",
    "- Creating Documents: Next, a loop runs through the dataframe (`df`), creating a `Document` object for each row. Each `Document` consists of the text from the 'markdown' column and metadata (tags, date, title, id). These documents are then collectively stored in a list named docs.\n",
    "- Building the Index: The `VectorStoreIndex.from_documents` function is then used to create an index `index_with_splitter` from the docs. This index is built within the provided `service_context_with_splitter`.\n",
    "- Persistence: Finally, the `index_with_splitter.storage_context.persist` method is called to save the index to a designated directory (`PERSISTENT_STORAGE`). This step ensures that the index is saved in a persistent storage, making it reusable and avoiding the need to rebuild the index from scratch in the future.\n",
    "\n",
    "### Loading from Persistent Storage\n",
    "\n",
    "- Rebuilding Storage Context: The `StorageContext.from_defaults` function is used to rebuild the `storage_context` from the `PERSISTENT_STORAGE` directory. This step prepares the environment to access the previously saved index.\n",
    "- Loading the Index: The `load_index_from_storage` function is then called with the rebuilt `storage_context` and `service_context_with_splitter`. It loads the previously saved index `index_with_splitter` from the persistent storage. This process is much faster than rebuilding the index from scratch, especially for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef401f-c26b-4734-8ab0-49ca901be160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(os.path.join(path_project,\"XX.parquet\")).reset_index(drop =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505b8df-61f8-4d8c-b31d-6ca4295905d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_persistante = False\n",
    "text_splitter = SentenceSplitter(chunk_size=600, chunk_overlap=90)\n",
    "### with\n",
    "\n",
    "service_context_with_splitter = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model, llm=llm, text_splitter=text_splitter\n",
    ")\n",
    "if create_persistante:\n",
    "    docs = []\n",
    "    ### Create the document\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        docs.append(\n",
    "            Document(\n",
    "                text=df[\"formatted_card\"][i], #### change to the column with the text information\n",
    "                metadata={\n",
    "                    \"communities\": list(df[\"clean_communities\"][i]), ### add tag\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "    # Create your index with the specified embedding model\n",
    "\n",
    "    index_with_splitter = VectorStoreIndex.from_documents(\n",
    "        docs, service_context=service_context_with_splitter\n",
    "    )\n",
    "\n",
    "    ### make it persistent\n",
    "\n",
    "    index_with_splitter.storage_context.persist(\n",
    "        persist_dir=os.path.join(path_project, folder_vector)\n",
    "    )\n",
    "else:\n",
    "    # rebuild storage context\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=os.path.join(path_project, folder_vector)\n",
    "    )\n",
    "\n",
    "    # load index\n",
    "\n",
    "    index_with_splitter = load_index_from_storage(\n",
    "        storage_context, service_context=service_context_with_splitter\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd76c70-1c03-4a9b-b025-a2cb776fff20",
   "metadata": {},
   "source": [
    "## How to naviguate through the doc store\n",
    "\n",
    "\n",
    "When dealing with the document store in Llama Index, understanding the structure and navigation is important. Each document is segmented into smaller components known as nodes, optimizing the indexing and retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70726e3-b5d0-4819-96a1-cb4db9b71ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index_with_splitter.docstore.docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c5718-66e9-4f06-95b2-432b453c308d",
   "metadata": {},
   "source": [
    "To understand what's contained within these nodes, you can fetch a specific node and examine its contents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198705a-a918-48d2-92e0-72984a9b0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_key = next(iter(index_with_splitter.docstore.docs))\n",
    "first_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15b0eb-9f62-4c24-adfd-d4d93db1374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_relationship = index_with_splitter.docstore.get_document(first_key)\n",
    "node_relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c91d9-54bc-438f-aed0-ff4dc379fb24",
   "metadata": {},
   "source": [
    "This node encapsulates not just the textual content but also valuable metadata such as tags, date, title, and ID. This rich information enhances the context and relevance of your search operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c4f8b-148c-4970-930a-44c8c837cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_relationship.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874660b5-5e72-44e3-8d97-e4075921fc1a",
   "metadata": {},
   "source": [
    "Nodes within the document store are not isolated; they are interconnected, forming a comprehensive network of text. Exploring these relationships allows you to navigate through the document store systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8a6c8-a5f7-4ea4-9c29-9eae34830b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_relationship.relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b45dafd-0a88-4a2d-9da8-28825e83ce00",
   "metadata": {},
   "source": [
    "To reconstruct a complete document or explore related content, you can traverse through these nodes by identifying and fetching parent nodes and associated nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf97d4-924f-4ca8-89aa-4aebbe1f7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "### parent= https://docs.llamaindex.ai/en/stable/api/llama_index.schema.NodeRelationship.html#id2\n",
    "parent = [values.node_id for key, values in node_relationship.relationships.items() if key.value == '1']\n",
    "parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2af1bc-2317-4318-9b97-0dc82b3b7af3",
   "metadata": {},
   "source": [
    "Now that we have the parent, we can find the related nodes, and reconstruct the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8028c-467c-468c-a3bb-324d5989577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = index_with_splitter.docstore.get_ref_doc_info(ref_doc_id = parent[0])\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f24f0-638b-4d63-80af-49d366bdcbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = index_with_splitter.docstore.get_nodes(node_ids = source.node_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3490b8-0c18-4a4c-a620-3e93c2cacaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### print the doc\n",
    "[print(i.text) for i in all_nodes]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "llama_index"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
