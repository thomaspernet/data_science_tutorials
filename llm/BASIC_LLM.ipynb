{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f3c30e-9dc5-4a16-bac5-f9175c0970f4",
   "metadata": {},
   "source": [
    "# How to use LLm\n",
    "\n",
    "\n",
    "Introduction to Leveraging Language Models with Hugging Face for Controlled Text Generation\n",
    "In this notebook, we delve into the practicalities of utilizing large language models (LLMs) through the Hugging Face Transformers library, focusing on generating text that aligns with specific requirements and constraints. Hugging Face provides a comprehensive suite of tools and models that facilitate easy access to state-of-the-art natural language processing (NLP) capabilities. One of the most powerful features of these models is their ability to generate coherent and contextually relevant text based on a given prompt.\n",
    "\n",
    "However, generating text that not only makes sense but also adheres to particular length, style, or content guidelines presents a unique set of challenges. Directly controlling the length of the generated text, for instance, can sometimes result in outputs that truncate awkwardly, cutting off sentences mid-thought or omitting crucial information. To navigate these challenges, we explore two primary strategies:\n",
    "\n",
    "- Prompt Engineering: A technique where the input prompt is carefully crafted to include explicit instructions or constraints, guiding the model towards generating text within desired parameters. While effective to a degree, the approach relies heavily on the model's interpretive capabilities and may not always produce consistently reliable results.\n",
    "- Custom Stopping Criteria: A more technical method that involves programming specific conditions under which text generation should cease. This approach allows for finer control over the endpoint of the generated text, aiming to ensure that outputs are both coherent and contextually complete without unnecessary truncation.\n",
    "\n",
    "### Sources:\n",
    "\n",
    "- https://huggingface.co/docs/transformers/en/llm_tutorial#generation-with-llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34aa19da-faf4-474c-8e95-4bbb8d112c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28555e17-ba97-4f46-8384-2ce6626e6bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cabc3a4e-d660-4b87-9cce-8850fa95e009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## choose model\n",
    "model = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f00bbf-7619-48c4-8583-031daf97e0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6799530-534f-43a7-9747-1e52d92763da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdeef6c92a2f41c4aa77e13fd0145b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### load base model\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "     model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b5345-ca03-4f88-b5ea-baed73a30a51",
   "metadata": {},
   "source": [
    "When working with language models from the Hugging Face Transformers library, selecting and properly configuring the tokenizer is a crucial step. The tokenizer is responsible for converting text into a format that the model can understand (i.e., converting text into tokens or token IDs) and vice versa (i.e., converting token IDs back into text). Special care must be taken when dealing with special characters and ensuring the tokenizer is correctly set up for the task at hand.\n",
    "\n",
    "This line of code initializes the tokenizer associated with the specified model. The `AutoTokenizer.from_pretrained` method automatically selects the correct tokenizer based on the given model identifier. This ensures compatibility between the model and the tokenizer, which is essential for effective model performance.\n",
    "\n",
    "Configuring the Tokenizer with Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a4802e-9782-4dd1-8b93-085eb8b82591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### select tokeniser : be carefull with the special characters\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9133ed-3bd6-4d2b-8ad6-bd6b994d2a3f",
   "metadata": {},
   "source": [
    "In some scenarios, especially when dealing with sequence generation tasks, it's important to ensure that the tokenizer has a defined padding token (pad_token). The padding token is used to fill in sequences to a uniform length, which is a requirement for certain models and training procedures.\n",
    "\n",
    "However, not all tokenizers come with a predefined padding token. In such cases, this snippet sets the tokenizer's padding token (pad_token) to the end-of-sequence token (eos_token) if it is not already defined. The end-of-sequence token is a special character that indicates the end of a text segment. By using the eos_token as a fallback for the pad_token, we ensure that the tokenizer can still perform padding operations when necessary, while maintaining compatibility with the model's expectations for input and output formats.\n",
    "\n",
    "Importance of Special Characters Handling\n",
    "Handling special characters, such as the end-of-sequence token, is crucial because it affects how the model interprets the start and end of texts. Misconfiguration can lead to suboptimal model performance, such as improper text generation boundaries or incorrect sequence lengths. Ensuring that all necessary special tokens are correctly set in the tokenizer configuration helps achieve more accurate and coherent outputs from the model.\n",
    "\n",
    "In summary, selecting the appropriate tokenizer and ensuring it is correctly configured with all necessary special tokens is essential for the successful application of language models to natural language processing tasks. This setup ensures that the model can accurately process input texts and generate meaningful and contextually appropriate outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f075fc-c0ab-487d-99a5-53749c60d2e4",
   "metadata": {},
   "source": [
    "## Understanding Prediction Parameters in Language Models\n",
    "\n",
    "### Key Parameters for Controlling Predictions:\n",
    "\n",
    "1. repetition_penalty\n",
    "- What It Does: Adjusts the likelihood of repeating the same words or phrases.\n",
    "- Explanation: A higher repetition_penalty discourages the model from repeating itself, leading to more diverse and varied outputs. It's like telling a storyteller to avoid saying the same thing over and over.\n",
    "2. max_length\n",
    "- What It Does: Sets the number of tokens (words or pieces of words) to generate.\n",
    "- Explanation: This parameter decides how long the output will be. For example, if nb_tokens is set to 50, the model will generate an output approximately 50 tokens long, akin to setting a word limit for an essay or a story.\n",
    "3. temperature\n",
    "- What It Does: Controls the randomness in the prediction process.\n",
    "- Explanation: A lower temperature (closer to 0) makes the model more likely to choose the most likely next word, resulting in more predictable text. A higher temperature increases randomness, leading to more creative and less predictable outputs. It's like adjusting how adventurous the model is in its language use.\n",
    "4. top_p (Top-p Sampling)\n",
    "- What It Does: Determines how to select the next word based on a probability threshold.\n",
    "- Explanation: With top_p sampling, the model considers the most likely next words that cumulatively reach the probability top_p. For instance, if top_p is 0.9, the model selects from the top words that together have a 90% chance of being the next word. It's like choosing the next word from a basket of the most likely candidates.\n",
    "Conclusion:\n",
    " - These parameters allow fine control over the language generation process, influencing the length, diversity, randomness, and predictability of the generated text. By adjusting these settings, you can tailor the model's outputs to your specific needs, whether you're aiming for concise, predictable text or longer, more creative passages.\n",
    "\n",
    "### Selecting Prediction Parameters for Different Scenarios\n",
    "\n",
    "1. For Concise and Focused Text:\n",
    " - repetition_penalty: High (e.g., 1.5 to 2.0) to avoid repetition.\n",
    " - max_length: Low to moderate (e.g., 50 to 100) for brevity.\n",
    " - temperature: Low (e.g., 0.3 to 0.7) for predictable, coherent output.\n",
    " - top_p: Moderate (e.g., 0.8) to balance creativity with coherence.\n",
    "2. For Creative and Diverse Text:\n",
    " - repetition_penalty: Moderate (e.g., 1.0 to 1.2) to allow some natural repetition.\n",
    " - max_length: Higher (e.g., 100 to 200) for extended content.\n",
    " - temperature: Higher (e.g., 0.7 to 1.0) for more randomness and creativity.\n",
    "- top_p: High (e.g., 0.9 or above) to include a wider range of word choices.\n",
    "3. For Generating Technical or Factual Content:\n",
    " - repetition_penalty: Moderate to high (e.g., 1.2 to 1.5) for clarity.\n",
    " - max_length: Adjust based on content length requirements.\n",
    " - temperature: Lower (e.g., 0.3 to 0.5) for more factual and straightforward content.\n",
    " - top_p: Lower to moderate (e.g., 0.6 to 0.8) to maintain relevance and accuracy.\n",
    "4. For Interactive Conversations or Chatbots:\n",
    " - repetition_penalty: Moderate (e.g., 1.1 to 1.3) to maintain a natural flow.\n",
    " - max_length: Moderate (e.g., 50 to 100) for manageable response lengths.\n",
    " - temperature: Moderate (e.g., 0.5 to 0.8) to balance predictability and spontaneity.\n",
    " - top_p: Moderate to high (e.g., 0.8 to 0.95) for varied but relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08c61c-8efc-42d9-89ba-8f025a4313c3",
   "metadata": {},
   "source": [
    "The function below generates an anwswer, by default Mistral returns the input prompt as part of the output. \n",
    "\n",
    "https://huggingface.co/docs/transformers/en/llm_tutorial#generated-output-is-too-shortlong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852258b-88fb-438e-8724-cb56a3c28b16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_response(prompt, model, tokenizer, **kwargs):\n",
    "    # Define an empty dictionary for model.generate() kwargs\n",
    "    generate_kwargs = {}\n",
    "\n",
    "    # Update the generate_kwargs with any provided kwargs\n",
    "    generate_kwargs.update(kwargs)\n",
    "\n",
    "    # Generate the output\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            **inputs,\n",
    "            **generate_kwargs\n",
    "        )[0], skip_special_tokens=True\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9684a8-1083-4be6-b521-3a727c8b2a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What is the definition of machine learning?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c9528-7c6f-4c8e-a6be-dba33d2e4ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_response(prompt, model_base, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2edd83-f1d4-4160-8ebd-5a2b972267f9",
   "metadata": {},
   "source": [
    "We can extract the answer only using the `input_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "242ad7a7-04ef-4d4c-aab6-a79a3382c473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_response_answer_only(prompt, model, tokenizer, **kwargs):\n",
    "    # Define an empty dictionary for model.generate() kwargs\n",
    "    generate_kwargs = {}\n",
    "    # Update the generate_kwargs with any provided kwargs\n",
    "    generate_kwargs.update(kwargs)\n",
    "    #eos_token_id = tokenizer.eos_token_id\n",
    "    # Generate the output\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    tokens = model.generate(\n",
    "            **inputs,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            #eos_token_id=eos_token_id,\n",
    "            **generate_kwargs\n",
    "        )\n",
    "    output = tokenizer.decode(tokens[0][inputs[\"input_ids\"].size(1) :], skip_special_tokens=True)\n",
    "    return {'answer':output, 'tokens':tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f2ae497-51e5-425f-a293-a1263009b7fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-thomas-pernet/.conda/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': '\\n\\nMachine learning is a subset of artificial intelligence (',\n",
       " 'tokens': tensor([[    1,  1824,   349,   272,  7526,   302,  5599,  5168, 28804,    13,\n",
       "             13, 15183,  5168,   349,   264, 19804,   302, 18278, 10895,   325]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response_answer_only(prompt, model_base, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9342f83-16b2-4409-aa41-43fed84362d2",
   "metadata": {},
   "source": [
    "The model truncates the sentences as soon as the generator reaches the number of token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "736771fe-bd84-4a88-a0f2-f6415eef4315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '\\n\\nMachine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves. The process of learning begins with observations or data, such as historical data, and then the machine learning model looks for patterns in the data and learns to identify them. The application of this technology can be seen in a variety of domains, including',\n",
       " 'tokens': tensor([[    1,  1824,   349,   272,  7526,   302,  5599,  5168, 28804,    13,\n",
       "             13, 15183,  5168,   349,   264, 19804,   302, 18278, 10895,   325,\n",
       "          11741, 28731,   369,  5312,  4918,   272,  5537,   298, 10226,  2822,\n",
       "            304,  4916,   477,  2659,  1671,  1250, 15956,  2007,  1591, 28723,\n",
       "            661, 21165,   356,   272,  4099,   302,  6074,  7034,   369,   541,\n",
       "           2735,  1178,   304,   938,   378,   298,  2822,   354,  3892, 28723,\n",
       "            415,  1759,   302,  5168, 10658,   395, 13875,   442,  1178, 28725,\n",
       "           1259,   390, 10578,  1178, 28725,   304,   868,   272,  5599,  5168,\n",
       "           2229,  4674,   354, 11533,   297,   272,  1178,   304,  2822, 28713,\n",
       "            298,  9051,   706, 28723,   415,  4993,   302,   456,  5514,   541,\n",
       "            347,  2598,   297,   264,  6677,   302, 25998, 28725,  2490]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### add more token\n",
    "get_response_answer_only(prompt, model_base, tokenizer, max_new_tokens = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92083215-912b-4513-8b8a-0a8e06e42309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '. Machine learning is a subset of artificial intelligence that utilizes statistical techniques and algorithms to enable computer systems to automatically learn and improve from experience without being explicitly programmed. It involves analyzing data to identify patterns and make predictions based on that data.',\n",
       " 'tokens': tensor([[    1,  1824,   349,   272,  7526,   302,  5599,  5168, 28804, 26307,\n",
       "            297, 28705, 28750, 23748, 28723, 13253,  5168,   349,   264, 19804,\n",
       "            302, 18278, 10895,   369,  4479,  5004, 21256,  9804,   304, 18539,\n",
       "            298,  8234,  6074,  4918,   298, 10226,  2822,   304,  4916,   477,\n",
       "           2659,  1671,  1250, 15956,  2007,  1591, 28723,   661, 14657, 10148,\n",
       "          14508,  1178,   298,  9051, 11533,   304,  1038, 20596,  2818,   356,\n",
       "            369,  1178, 28723,     2]], device='cuda:0')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the definition of machine learning? Answer in 2 sentences\"\n",
    "get_response_answer_only(prompt, model_base, tokenizer, max_new_tokens = 100, temperature = .4,do_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef3bb0-81a7-4ebd-b2b0-0453603ea4a6",
   "metadata": {},
   "source": [
    "When generating text using the Hugging Face Transformers library, controlling the output length is crucial for aligning the generated content with specific requirements. However, directly specifying the length of the generated text can sometimes lead to outputs that abruptly truncate, potentially cutting off sentences mid-way or omitting relevant information. Here, we explore strategies to manage text generation length effectively while aiming to preserve the coherence and completeness of the generated text.\n",
    "\n",
    "Direct Length Specifications\n",
    "- max_length: Defines the total maximum length of the output sequence, including the length of input_ids. This hard limit can result in abrupt endings if the limit is reached mid-sentence.\n",
    "- min_length: Sets the minimum length of the generated sequence. The generation process will not stop until at least this length is reached, providing a lower bound to ensure a minimum output size.\n",
    "- max_new_tokens: Specifies the maximum number of new tokens to generate, exclusive of the input length. This parameter allows for more predictable control over the size of the generated addition but can still lead to truncated sentences.\n",
    "\n",
    "Overcoming Truncation Issues\n",
    "\n",
    "To address the limitations of direct length specifications and improve the reliability of generating coherent and complete sentences, two main approaches can be considered:\n",
    "\n",
    "\n",
    "- Prompt engineering involves embedding explicit instructions within the input to guide the model's output, \n",
    "- custom stopping criteria utilize programmable conditions to more precisely control over the text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb056f-29c6-400b-bfbf-9c928e552ef4",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "- Description: Incorporating explicit instructions into the input prompt, such as \"Answer in 3 sentences,\" can guide the language model (LLM) to generate text within a desired length or format.\n",
    "- Limitations: While this method can influence the model's output, it does not guarantee strict adherence to the instructions. The model might generate more or fewer sentences than requested, reflecting the inherent unpredictability in how LLMs interpret and follow such instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31ea4c0b-15d5-4393-8d2f-4f7a963c8394",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"Machine learning is a subset of artificial intelligence that enables systems to automatically learn and improve from experience without being explicitly programmed. It's based on algorithms and statistical models that analyze data and recognize patterns to make predictions or take actions. Machine learning applications include email filtering, recommendation systems, fraud detection, and self-driving cars.\",\n",
       " 'tokens': tensor([[    1,  1824,   349,   272,  7526,   302,  5599,  5168, 28804, 26307,\n",
       "            297, 28705, 28770, 23748, 28723, 13253,  5168,   349,   264, 19804,\n",
       "            302, 18278, 10895,   369, 18156,  4918,   298, 10226,  2822,   304,\n",
       "           4916,   477,  2659,  1671,  1250, 15956,  2007,  1591, 28723,   661,\n",
       "          28742, 28713,  2818,   356, 18539,   304, 21256,  4994,   369, 20765,\n",
       "           1178,   304, 11286, 11533,   298,  1038, 20596,   442,  1388,  6768,\n",
       "          28723, 13253,  5168,  8429,  3024,  4927,  5531,   288, 28725, 26077,\n",
       "           4918, 28725, 17764, 15109, 28725,   304,  1008, 28733, 28715, 16982,\n",
       "           8300, 28723,     2]], device='cuda:0')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the definition of machine learning? Answer in 3 sentences.\"\n",
    "get_response_answer_only(prompt, model_base, tokenizer, max_new_tokens = 100, temperature = .4,do_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fa2a4-1cdf-4020-b95c-21f29b71b5fb",
   "metadata": {},
   "source": [
    "When working with the mistralai/Mistral-7B-Instruct-v0.2 model and its corresponding tokenizer, it's important to understand how punctuation marks and special sequence tokens are handled and represented. This understanding is crucial for tasks such as setting up stopping criteria for text generation based on specific punctuation marks or tokens.\n",
    "\n",
    "**Special Tokens**\n",
    "\n",
    "`<s>` and `</s>`: These are special tokens used to denote the start (`<s>`) and end (`</s>`) of a sequence. They are crucial for the model to understand the boundaries of the text being processed or generated.\n",
    "\n",
    "**Punctuation Marks**\n",
    "\n",
    "The tokenizer assigns unique token IDs to punctuation marks, treating them as distinct tokens within its vocabulary. For example, periods (.) and commas (,) are recognized and tokenized with specific IDs, allowing for precise control over text generation processes, such as stopping generation upon encountering these tokens.\n",
    "\n",
    "**Example: Token ID Inspection**\n",
    "\n",
    "To understand how specific tokens, including punctuation marks and special tokens, are represented within the tokenizer's vocabulary, you can convert token IDs back to their textual representation using the tokenizer's convert_ids_to_tokens method. This is particularly useful for setting up stopping criteria based on these tokens.\n",
    "\n",
    "- Token ID 1 corresponds to the start-of-sequence token `<s>`.\n",
    "- Token ID 28723 corresponds to the period (.) punctuation mark.\n",
    "- Token ID 28725 corresponds to the comma (,) punctuation mark.\n",
    "- Token ID 2 corresponds to the end-of-sequence token `</s>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "788e94fe-a8fc-4100-84ea-b38b285db7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '.', ',', '</s>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example code to check what specific tokens represent\n",
    "token_ids_to_check = [1,28723, 28725, 2]\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids_to_check)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2c80a-ba43-47bd-b85e-fc84327c2788",
   "metadata": {},
   "source": [
    "And to get the ids from tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31772a96-b78c-4c49-bac3-2b9499025df6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 28723, 28725, 2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118959a8-ba20-4e8a-8240-9e0e3144dd19",
   "metadata": {},
   "source": [
    "## Stopping criteria\n",
    "\n",
    "- Implementation: As demonstrated with the ConditionalStoppingCriteria class, custom stopping criteria can be programmed to stop text generation based on specific conditions, such as the occurrence of punctuation marks that signify the end of sentences.\n",
    "- Advantages: This method provides a more nuanced control over the generation process, allowing for stopping at natural concluding points in the text.\n",
    "- Challenges: One potential downside is that relevant information might be part of the subsequent sentence that gets partially generated before stopping. This can result in the loss of valuable context or details that would have been included if the sentence had been allowed to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf2b83dd-0480-4151-9fae-c68c923186e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class CustomStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, stopping_ids_list):\n",
    "        super().__init__()\n",
    "        self.stopping_ids_list = stopping_ids_list\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Check if the last generated token is in the stopping_ids_list\n",
    "        return input_ids[0, -1].item() in self.stopping_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2b0d864-00cd-494a-b912-4e6d7b2f59a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28723, 28725, 28804, 28808]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the token IDs for the punctuation marks you want to stop at\n",
    "punctuation_tokens = ['.', ',', '?', '!']  # Define the tokens\n",
    "stopping_ids = tokenizer.convert_tokens_to_ids(punctuation_tokens)  # Convert tokens to their corresponding IDs\n",
    "stopping_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6082b959-0834-4891-ba87-54c8312d3966",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28723, 28725, 28804, 28808]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the custom stopping criteria with these IDs\n",
    "stopping_criteria = CustomStoppingCriteria(stopping_ids_list=stopping_ids)\n",
    "stopping_criteria.stopping_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9cc51b0-53fe-49c5-bab7-fdc3b6a06018",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the definition of machine learning?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "output_sequences = model_base.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,  # Adjust based on your needs\n",
    "    temperature=0.4,\n",
    "    do_sample= True,\n",
    "    stopping_criteria=StoppingCriteriaList([stopping_criteria])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b7205f3-d14b-4cd0-975a-4446965ffeea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  1824,   349,   272,  7526,   302,  5599,  5168, 28804,    13,\n",
       "            13, 15183,  5168,   349,   264, 19804,   302, 18278, 10895,   369,\n",
       "          5312,  4918,   272,  5537,   298, 10226,  2822,   304,  4916,   477,\n",
       "          2659,  1671,  1250, 15956,  2007,  1591, 28723]], device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5ac7efb4-be7a-40fe-b043-8077350597c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMachine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_sequences[0][inputs[\"input_ids\"].size(1) :], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc56570-9899-489c-b6c7-55dfe8454a6f",
   "metadata": {},
   "source": [
    "### More flexibility\n",
    "\n",
    "The code below explicitelly tells how many sentences we want to generate within the max tokens allow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c14cb2e1-ea97-462c-9f04-70ab8b68e2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "398a0660-11ba-457d-8378-8873802ed002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConditionalStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, stopping_rules):\n",
    "        super().__init__()\n",
    "        # Convert token rules to their corresponding IDs and occurrence counts\n",
    "        self.stopping_ids_counts = {tokenizer.convert_tokens_to_ids(token): count for token, count in stopping_rules.items()}\n",
    "        # Initialize a counter to track occurrences of each token ID\n",
    "        self.token_occurrences = defaultdict(int)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Update occurrences for the last generated token\n",
    "        last_token_id = input_ids[0, -1].item()\n",
    "        if last_token_id in self.stopping_ids_counts:\n",
    "            self.token_occurrences[last_token_id] += 1\n",
    "            # Check if the occurrence count meets or exceeds the required count for stopping\n",
    "            if self.token_occurrences[last_token_id] >= self.stopping_ids_counts[last_token_id]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "stopping_rules = {\n",
    "    '.': 2,  # Stop after generating 2 sentences (or encountering '.' twice)\n",
    "}\n",
    "stopping_criteria = ConditionalStoppingCriteria(tokenizer, stopping_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0fcfe0d5-e24a-45f8-b2d4-d35c4b6d9067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What is the definition of machine learning?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b5a72-2183-4a41-a939-91cd3b43b2ca",
   "metadata": {},
   "source": [
    "Without criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e48b7ed9-7852-4320-8e67-0abd3c196e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output_sequences = model_base.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,  # Adjust based on your needs\n",
    "    temperature=0.4,\n",
    "    do_sample= True,\n",
    "    stopping_criteria=StoppingCriteriaList([stopping_criteria])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9a8d511e-8400-4043-b554-fa97c05082d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  1824,   349,   272,  7526,   302,  5599,  5168, 28804,    13,\n",
       "            13, 15183,  5168,   349,   264, 19804,   302, 18278, 10895,   325,\n",
       "         11741, 28731,   369,  5312,  4918,   272,  5537,   298, 10226,  2822,\n",
       "           304,  4916,   477,  2659,  1671,  1250, 15956,  2007,  1591, 28723,\n",
       "           661, 21165,   356,   272,  4099,   302,  6074,  7034,   369,   541,\n",
       "          2735,  1178,   304,   938,   378,   298,  2822,   354,  3892, 28723]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "739bc8fb-4e14-4448-bc2b-2e1b9a5c7bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMachine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_sequences[0][inputs[\"input_ids\"].size(1) :], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9265c4-4f86-4e22-844b-0b71dfdfb99e",
   "metadata": {},
   "source": [
    "With criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb8e8702-d0fc-4524-a0f2-76297e941940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  1824,   349,   272,  7526,   302,  5599,  5168, 28804,    13,\n",
       "            13, 15183,  5168,   349,   264,  2038,   302,  1178,  5643,   369,\n",
       "          4607,  1002,   272,  3667,   302, 13305,   745,  4994, 28723,   661,\n",
       "         28742, 28713,  2818,   356,   272,  3028,   369,  4918,   541,  2822,\n",
       "           477,  1178, 28725,  9051, 11533,   304,  1038,  9549,   395, 13383,\n",
       "          2930, 20288, 28723, 13253,  5168, 18539,   938, 21256,  9804,   298,\n",
       "          8234,   272,  5599,   298,  2822,   477,  1178, 28725,  9051, 11533,\n",
       "           304,  1038,  9549,   395, 13383,  2930, 20288, 28723,   415,  1759,\n",
       "           302,  5168, 10658,   395, 13875,   442,  1178, 28725,  1259,   390,\n",
       "         10578,  1178,   442,  1178,   477, 16082,  8309, 28725,   690,   349,\n",
       "           868, 28649,   298,  9051, 11533,   304, 17869, 28723,   415,  5599,\n",
       "          5168,  2229,   349, 10898,   356,   456,  1178,   304,   541,   868,\n",
       "           347,  1307,   298,  1038, 20596,   442,  9549,   356,   633,  1178,\n",
       "         28723, 13253,  5168,   349,  1307,   297,   264,  5335,  2819,   302,\n",
       "          8429, 28725,   477,  4927,  5531,   288,   304,  6074,  8021,   298,\n",
       "          5593, 19912,   288,   304,  5714, 21967, 28723,     2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences = model_base.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,  # Adjust based on your needs\n",
    "    temperature=0.4,\n",
    "    do_sample= True\n",
    ")\n",
    "output_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d5e4453f-f2a3-4cd3-bf76-a6b001053467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nMachine learning is a method of data analysis that automates the building of analytical models. It's based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. Machine learning algorithms use statistical techniques to enable the machine to learn from data, identify patterns and make decisions with minimal human intervention. The process of learning begins with observations or data, such as historical data or data from sensor devices, which is then analyzed to identify patterns and trends. The machine learning model is trained on this data and can then be used to make predictions or decisions on new data. Machine learning is used in a wide range of applications, from email filtering and computer vision to financial forecasting and medical diagnosis.\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output_sequences[0][inputs[\"input_ids\"].size(1) :], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6685b-8f18-441b-b956-47d14ab84fa7",
   "metadata": {},
   "source": [
    "## Wrap everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "186e49b8-406b-44fb-8bdf-589647f75f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_response_answer_only(prompt, model, tokenizer, stopping_rules=None, **kwargs):\n",
    "    # Define an empty dictionary for model.generate() kwargs\n",
    "    generate_kwargs = {}\n",
    "    # Update the generate_kwargs with any provided kwargs\n",
    "    generate_kwargs.update(kwargs)\n",
    "    \n",
    "    # Setup conditional stopping criteria if stopping rules are provided\n",
    "    if stopping_rules:\n",
    "        stopping_criteria = ConditionalStoppingCriteria(tokenizer, stopping_rules)\n",
    "        generate_kwargs['stopping_criteria'] = StoppingCriteriaList([stopping_criteria])\n",
    "    \n",
    "    # Generate the output\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    tokens = model.generate(\n",
    "        **inputs,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        **generate_kwargs\n",
    "    )\n",
    "    output = tokenizer.decode(tokens[0][inputs[\"input_ids\"].size(1) :], skip_special_tokens=True)\n",
    "    return {'answer':output, 'tokens':tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a317d6ad-6e0b-466c-b404-1f67a50e0c11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMachine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the definition of machine learning?\"\n",
    "stopping_rules = {\n",
    "    '.': 1,  # Example rule: stop after 2 periods\n",
    "}\n",
    "response = get_response_answer_only(\n",
    "    prompt,\n",
    "    model_base, \n",
    "    tokenizer,\n",
    "    stopping_rules=stopping_rules,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.4, \n",
    "    do_sample=True\n",
    ")\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "acfc62a8-11f0-4cb2-9e7a-8925de51fefa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nMachine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the definition of machine learning?\"\n",
    "stopping_rules = {\n",
    "    '.': 2,  # Example rule: stop after 2 periods\n",
    "}\n",
    "response = get_response_answer_only(\n",
    "    prompt,\n",
    "    model_base, \n",
    "    tokenizer,\n",
    "    stopping_rules=stopping_rules,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.4, \n",
    "    do_sample=True\n",
    ")\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b6769417-d1f9-43e3-82bd-aecfbce1d86d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves. The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for inherent patterns in data and make better decisions in the future based on the examples that we provide.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the definition of machine learning?\"\n",
    "stopping_rules = {\n",
    "    '.': 3,  # Example rule: stop after 2 periods\n",
    "}\n",
    "response = get_response_answer_only(\n",
    "    prompt,\n",
    "    model_base, \n",
    "    tokenizer,\n",
    "    stopping_rules=stopping_rules,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.4, \n",
    "    do_sample=True\n",
    ")\n",
    "response['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
